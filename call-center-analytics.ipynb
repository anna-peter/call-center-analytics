{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Call transcription and analysis with Azure OpenAI \n",
    "This demo notebook shows how to perform analysis on a Swiss German audio interaction between two parties. Example use cases include the analysis of call center customer interactions. We provide the following capabilities:\n",
    "- sentiment analysis\n",
    "- general summarization of the call\n",
    "- summarization of each speaker's remarks with speaker recognition\n",
    "\n",
    "Before running the cells, create environment variables in a .env file for the openAI API key and endpoint and the speech API key. There are some packages that can be installed via ``pip install``. \n",
    "\n",
    "Check here for the official GitHub repo including other samples: [GitHub - Azure OpenAI Samples](https://github.com/Azure/azure-openai-samples/blob/main/use_cases/call_center/notebooks/call_center.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import json, os\n",
    "import string\n",
    "import time\n",
    "import wave\n",
    "\n",
    "import openai\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "from num2words import num2words\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY') \n",
    "openai.api_base = os.getenv('OPENAI_API_ENDPOINT') \n",
    "openai.api_version = '2023-05-15'\n",
    "\n",
    "SPEECH_KEY = os.getenv(\"SPEECH_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_speech_from_file(filename):\n",
    "    # Set up the subscription info for the Speech Service:\n",
    "    # Replace with your own subscription key and service region (e.g., \"westus\").\n",
    "    speech_key = SPEECH_KEY\n",
    "    service_region = \"westeurope\"\n",
    "\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "    audio_config = speechsdk.audio.AudioConfig(filename=filename)\n",
    "    # Creates a speech recognizer using a file as audio input, also specify the speech language\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, language = \"de-CH\",audio_config=audio_config)\n",
    "\n",
    "    global done \n",
    "    done = False\n",
    "    global recognized_text_list \n",
    "    recognized_text_list=[]\n",
    "    def stop_cb(evt: speechsdk.SessionEventArgs):\n",
    "        \"\"\"callback that signals to stop continuous recognition upon receiving an event `evt`\"\"\"\n",
    "        print('CLOSING on {}'.format(evt))\n",
    "        global done\n",
    "        done = True\n",
    "\n",
    "    def recognize_cb(evt: speechsdk.SpeechRecognitionEventArgs):\n",
    "        \"\"\"callback for recognizing the recognized text\"\"\"\n",
    "        global recognized_text_list\n",
    "        recognized_text_list.append(evt.result.text)\n",
    "        # print('RECOGNIZED: {}'.format(evt.result.text))\n",
    "\n",
    "    # Connect callbacks to the events fired by the speech recognizer\n",
    "    # speech_recognizer.recognizing.connect(lambda evt: print('RECOGNIZING: {}'.format(evt)))\n",
    "    speech_recognizer.recognized.connect(recognize_cb)\n",
    "    speech_recognizer.session_started.connect(lambda evt: print('STT SESSION STARTED: {}'.format(evt)))\n",
    "    speech_recognizer.session_stopped.connect(lambda evt: print('STT SESSION STOPPED {}'.format(evt)))\n",
    "    # speech_recognizer.canceled.connect(lambda evt: print('CANCELED {}'.format(evt)))\n",
    "    # stop continuous recognition on either session stopped or canceled events\n",
    "    speech_recognizer.session_stopped.connect(stop_cb)\n",
    "    # speech_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "    # Start continuous speech recognition\n",
    "    speech_recognizer.start_continuous_recognition()\n",
    "    while not done:\n",
    "        time.sleep(.5)\n",
    "\n",
    "    speech_recognizer.stop_continuous_recognition()\n",
    "\n",
    "    return recognized_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = recognize_speech_from_file(\"name_of_your_audio_file.wav\")\n",
    "print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Prompt for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_sentiment = f\"Detect whether the sentiment of the customer is positive or negative. Just say positive or negative.\\n\\n{' '.join(text)}\"\n",
    "\n",
    "deployment_name=\"name_of_your_openai_deployment\"\n",
    "\n",
    "openai.Completion.create(engine=deployment_name,prompt=prompt_sentiment,temperature=0,max_tokens=30,)[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Prompt for Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_summary = f\"Summarize the contents of the customer call.\\n\\n{' '.join(text)}\"\n",
    "\n",
    "result = openai.Completion.create(engine=deployment_name,prompt=prompt_summary,temperature=0,max_tokens=300,)[\"choices\"][0][\"text\"].strip(\" \\n\")\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Prompt for Speaker Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_test = f\"Split the conversation by the two speakers and summarize what each speaker said.\\n\\n{' '.join(text)}\"\n",
    "\n",
    "result = openai.Completion.create(engine=deployment_name,prompt=prompt_test,temperature=0,max_tokens=600,)[\"choices\"][0][\"text\"].strip(\" \\n\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
