{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Post Call Transcription and Analysis with Azure OpenAI \n",
        "This demo notebook shows how to perform analysis on a Swiss German audio interaction between two parties. The audio file is downloaded from an Azure blob store. Example use cases include the analysis of call center customer interactions. We provide the following capabilities:\n",
        "- sentiment analysis\n",
        "- general summarization of the call\n",
        "- summarization of each speaker's remarks with speaker recognition\n",
        "\n",
        "Before running the cells, create environment variables in a .env file for the openAI API key and endpoint and the speech API key. There are some packages that need to be installed via ``%pip install`` (for example python-dotenv, azure-cognitiveservices-speech, openai)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686644405662
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Workspace, Model, Dataset, Datastore, Experiment, Environment, ScriptRunConfig, RunConfiguration\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "from azureml.pipeline.core import Pipeline\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "from azureml.core.environment import CondaDependencies\n",
        "import joblib\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import logging\n",
        "import requests\n",
        "from azure.identity import ChainedTokenCredential,ManagedIdentityCredential,DefaultAzureCredential\n",
        "from azure.storage.filedatalake import DataLakeServiceClient\n",
        "import azureml.core\n",
        "from time import sleep\n",
        "import time\n",
        "import azure.functions as func\n",
        "import json \n",
        "from azure.storage.blob import BlobServiceClient, ContainerClient, BlobBlock, BlobClient\n",
        "import azure.cognitiveservices.speech as speechsdk\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "import string\n",
        "import openai\n",
        "import sys\n",
        "import numpy as np\n",
        "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
        "import io\n",
        "from pydub import AudioSegment\n",
        "\n",
        "openai.api_type = \"azure\"\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY') \n",
        "openai.api_base = os.getenv('OPENAI_API_ENDPOINT') \n",
        "openai.api_version = '2023-05-15'\n",
        "\n",
        "SPEECH_KEY = os.getenv(\"SPEECH_API_KEY\")\n",
        "print('SDK version:', azureml.core.VERSION)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Fetch the Audio from Azure Blob Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1686645134415
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def download_file_from_blob(account_url, container_name, blob_name, out_file):\n",
        "    credential = DefaultAzureCredential()\n",
        "    # create the blobserviceclient object\n",
        "    blob_service_client = BlobServiceClient(account_url, credential=credential)\n",
        "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
        "    stream = io.BytesIO()\n",
        "    file = blob_client.download_blob().readall()\n",
        "    with open(out_file, mode='bx') as f:\n",
        "        f.write(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1686645138947
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "download_file_from_blob(\"https://<name of blob store>.blob.core.windows.net\", \"<name of container>\", \"<file path>\", \"<name of output file>.wav\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Speech to Text via Cognitive Services"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1686643913293
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def recognize_speech_from_file(filename):\n",
        "    # Set up the subscription info for the Speech Service:\n",
        "    # Replace with your own subscription key and service region (e.g., \"westus\").\n",
        "    speech_key = SPEECH_KEY\n",
        "    service_region = \"westeurope\"\n",
        "\n",
        "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
        "    audio_config = speechsdk.audio.AudioConfig(filename=filename)\n",
        "    # Creates a speech recognizer using a file as audio input, also specify the speech language\n",
        "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, language = \"de-CH\",audio_config=audio_config)\n",
        "\n",
        "    global done \n",
        "    done = False\n",
        "    global recognized_text_list \n",
        "    recognized_text_list=[]\n",
        "    def stop_cb(evt: speechsdk.SessionEventArgs):\n",
        "        \"\"\"callback that signals to stop continuous recognition upon receiving an event `evt`\"\"\"\n",
        "        print('CLOSING on {}'.format(evt))\n",
        "        global done\n",
        "        done = True\n",
        "\n",
        "    def recognize_cb(evt: speechsdk.SpeechRecognitionEventArgs):\n",
        "        \"\"\"callback for recognizing the recognized text\"\"\"\n",
        "        global recognized_text_list\n",
        "        recognized_text_list.append(evt.result.text)\n",
        "        # print('RECOGNIZED: {}'.format(evt.result.text))\n",
        "\n",
        "    # Connect callbacks to the events fired by the speech recognizer\n",
        "    # speech_recognizer.recognizing.connect(lambda evt: print('RECOGNIZING: {}'.format(evt)))\n",
        "    speech_recognizer.recognized.connect(recognize_cb)\n",
        "    speech_recognizer.session_started.connect(lambda evt: print('STT SESSION STARTED: {}'.format(evt)))\n",
        "    speech_recognizer.session_stopped.connect(lambda evt: print('STT SESSION STOPPED {}'.format(evt)))\n",
        "    # speech_recognizer.canceled.connect(lambda evt: print('CANCELED {}'.format(evt)))\n",
        "    # stop continuous recognition on either session stopped or canceled events\n",
        "    speech_recognizer.session_stopped.connect(stop_cb)\n",
        "    # speech_recognizer.canceled.connect(stop_cb)\n",
        "\n",
        "    # Start continuous speech recognition\n",
        "    speech_recognizer.start_continuous_recognition()\n",
        "    while not done:\n",
        "        time.sleep(.5)\n",
        "\n",
        "    speech_recognizer.stop_continuous_recognition()\n",
        "\n",
        "    return recognized_text_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686646060375
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "text = recognize_speech_from_file(\"<name of output file>.wav\")\n",
        "print(text)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Generate Responses via Azure OpenAI"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Create a Prompt for Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686646134447
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "prompt_sentiment = f\"Detect whether the sentiment of the customer is positive or negative. Just say positive or negative.\\n\\n{' '.join(text)}\"\n",
        "\n",
        "deployment_name=\"text-davinci-003-demo-we\"\n",
        "\n",
        "result = openai.Completion.create(engine=deployment_name,prompt=prompt_sentiment,temperature=0,max_tokens=30,)[\"choices\"][0][\"text\"].strip(\" \\n\")\n",
        "print(result)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Create a Prompt for Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686645444947
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "prompt_summary = f\"Summarize the contents of the customer call.\\n\\n{' '.join(text)}\"\n",
        "deployment_name=\"text-davinci-003-demo-we\"\n",
        "\n",
        "result = openai.Completion.create(engine=deployment_name,prompt=prompt_summary,temperature=0,max_tokens=3000,)[\"choices\"][0][\"text\"].strip(\" \\n\")\n",
        "print(result)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Create a Prompt for Speaker Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1686645461952
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "prompt_test = f\"Split the conversation by the two speakers and summarize in german what each speaker said.\\n\\n{' '.join(text)}\"\n",
        "deployment_name=\"text-davinci-003-demo-we\"\n",
        "\n",
        "result = openai.Completion.create(engine=deployment_name,prompt=prompt_test,temperature=0,max_tokens=3000,)[\"choices\"][0][\"text\"].strip(\" \\n\")\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
